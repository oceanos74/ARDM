{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchfly\n",
    "torchfly.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from apex import amp\n",
    "from allennlp.training.checkpointer import Checkpointer\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule, GPT2Tokenizer\n",
    "\n",
    "from torchfly.criterions import SequenceFocalLoss, SequenceCrossEntropyLoss\n",
    "from gpt_model import GPT2SimpleLM\n",
    "from cam676_eval.cam676_eval import clean_sentence, entities, entity_dict, success_f1_metric, bleu_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.sep_token = \"None\"\n",
    "# add speicial tokens in the same order as Roberta\n",
    "tokenizer.add_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = GPT2SimpleLM(GPT2SmallConfig)\n",
    "model_B = GPT2SimpleLM(GPT2SmallConfig)\n",
    "model_A.load_state_dict(torch.load(\"gpt2_small.pth\"))\n",
    "model_B.load_state_dict(torch.load(\"gpt2_small.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_keep_indices(batch_keep_indices):\n",
    "    prev = batch_keep_indices[1]\n",
    "    new_batch_keep_indices = [prev]\n",
    "\n",
    "    for i in range(1, len(batch_keep_indices)):\n",
    "        curr = batch_keep_indices[i]\n",
    "        new = []\n",
    "\n",
    "        for idx in curr:\n",
    "            new.append(prev.index(idx))\n",
    "\n",
    "        new_batch_keep_indices.append(new)\n",
    "        prev = curr\n",
    "        \n",
    "    return new_batch_keep_indices\n",
    "\n",
    "\n",
    "class CamRestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bos = tokenizer.encode(\"<s>\")\n",
    "        self.user_bos = tokenizer.encode(\"A:\")\n",
    "        self.system_bos = tokenizer.encode(\"B:\")\n",
    "        \n",
    "        self.eos = [628, 198]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_dialog = self.data[index]\n",
    "        \n",
    "        full_dialog_tokens = []\n",
    "        cur_pos = 0\n",
    "        \n",
    "        for turn_dialog in full_dialog:\n",
    "            # user\n",
    "            user_tokens = self.user_bos + tokenizer.encode(turn_dialog['user']) + self.eos\n",
    "            user_pos = torch.arange(cur_pos, cur_pos + len(user_tokens))\n",
    "            cur_pos = user_pos[-1] + 1\n",
    "            \n",
    "            # belief span\n",
    "            belief_tokens = self.bos + \\\n",
    "                            tokenizer.encode(\";\".join(turn_dialog['bspan_inform'][1:])) + \\\n",
    "                            self.eos\n",
    "            belief_pos = torch.arange(cur_pos, cur_pos + len(belief_tokens))\n",
    "            cur_pos = belief_pos[-1]\n",
    "            \n",
    "            # system\n",
    "            if np.random.rand() < 0.04:\n",
    "                turn_dialog[\"degree\"] = 0 \n",
    "            database = tokenizer.encode(str(turn_dialog[\"degree\"]))\n",
    "            # database_pos = torch.LongTensor([1023])\n",
    "            \n",
    "            system_tokens = self.system_bos + \\\n",
    "                            tokenizer.encode(turn_dialog['replaced_response']) + \\\n",
    "                            self.eos\n",
    "            system_pos = torch.arange(cur_pos, cur_pos + len(system_tokens) + 1)\n",
    "            cur_pos = system_pos[-1] + 1\n",
    "            \n",
    "            # concat database and response\n",
    "            system_tokens = database + system_tokens\n",
    "            # system_pos = torch.cat([database_pos, system_pos], dim=0)\n",
    "            \n",
    "            user_tokens = torch.LongTensor(user_tokens)\n",
    "            system_tokens = torch.LongTensor(system_tokens)\n",
    "            belief_tokens = torch.LongTensor(belief_tokens)\n",
    "            \n",
    "            full_dialog_tokens.append((user_tokens, \n",
    "                                       user_pos, \n",
    "                                       system_tokens, \n",
    "                                       system_pos, \n",
    "                                       belief_tokens, \n",
    "                                       belief_pos))\n",
    "\n",
    "        return full_dialog_tokens\n",
    "        \n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = self.tokenizer.encode(\"<pad>\")[0]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "\n",
    "        max_turn_len = max([len(item) for item in unpacked_data])\n",
    "        \n",
    "        batch_dialogs = []\n",
    "        batch_keep_indices = []\n",
    "\n",
    "        for turn_num in range(max_turn_len):\n",
    "\n",
    "            keep_indices = []\n",
    "\n",
    "            for batch_idx in range(len(unpacked_data)):\n",
    "                if turn_num < len(unpacked_data[batch_idx]):\n",
    "                    keep_indices.append(batch_idx)\n",
    "\n",
    "            user_tokens = pad_sequence([unpacked_data[idx][turn_num][0] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            user_pos = pad_sequence([unpacked_data[idx][turn_num][1] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            system_tokens = pad_sequence([unpacked_data[idx][turn_num][2] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            system_pos = pad_sequence([unpacked_data[idx][turn_num][3] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            belief_tokens = pad_sequence([unpacked_data[idx][turn_num][4] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            belief_pos = pad_sequence([unpacked_data[idx][turn_num][5] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)  \n",
    "\n",
    "            user_mask = (user_tokens != self.pad).byte()\n",
    "            system_mask = (system_tokens != self.pad).byte()\n",
    "            belief_mask = (belief_tokens != self.pad).byte()\n",
    "\n",
    "\n",
    "            batch_dialogs.append((user_tokens, user_pos, user_mask, \n",
    "                                  system_tokens, system_pos, system_mask, \n",
    "                                  belief_tokens, belief_pos, belief_mask))\n",
    "            batch_keep_indices.append(keep_indices)\n",
    "            \n",
    "        # align keep indices\n",
    "        # batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "        return batch_dialogs, batch_keep_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, target, mask):\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    target = target[:, 1:].contiguous()\n",
    "    mask = mask[:, 1:].contiguous().float()\n",
    "    loss = criterion(logits, target, mask, label_smoothing=0.02, reduce=True)\n",
    "    return loss\n",
    "\n",
    "def filter_past(past, keep_indices):\n",
    "    past = [item[:, keep_indices] for item in past]\n",
    "    return past\n",
    "\n",
    "def replace_punc(x):\n",
    "    x = x.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    return x.replace(\".\", \" .\").replace(\",\", \" .\").replace(\"?\", \" ?\").replace(\"?\", \" ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"../../data/DataProcess/train_data.pkl\")\n",
    "val_data = torch.load(\"../../data/DataProcess/val_data.pkl\")\n",
    "test_data = torch.load(\"../../data/DataProcess/test_data.pkl\")\n",
    "\n",
    "indices = np.arange(len(train_data))\n",
    "np.random.shuffle(indices)\n",
    "# use all data\n",
    "indices = indices[: 200]\n",
    "train_data = [train_data[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CamRestDataset(train_data, tokenizer)\n",
    "val_dataset = CamRestDataset(val_data, tokenizer)\n",
    "test_dataset = CamRestDataset(test_data, tokenizer)\n",
    "\n",
    "train_batch_size = 1\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True,\n",
    "                              batch_size=train_batch_size, \n",
    "                              collate_fn=collate_func)\n",
    "\n",
    "eval_batch_size = 16\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SequenceFocalLoss(gamma=0.0, beta=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = Checkpointer(serialization_dir=\"Checkpoint\", \n",
    "                            keep_serialized_model_every_num_seconds=3600*2, \n",
    "                            num_serialized_models_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "num_epochs = 10\n",
    "num_gradients_accumulation = 1\n",
    "num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // train_batch_size // num_gradients_accumulation\n",
    "\n",
    "param_optimizer = list(model_A.named_parameters()) + list(model_B.named_parameters())\n",
    "no_decay = ['ln', 'bias', 'LayerNorm']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=5e-5,\n",
    "                  correct_bias=False)\n",
    "\n",
    "scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                 warmup_steps=500,\n",
    "                                 t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level=\"O0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_weight = 1.0\n",
    "\n",
    "def train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False):\n",
    "\n",
    "    aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "   \n",
    "    mask = torch.ByteTensor([]).to(device)\n",
    "    prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "    \n",
    "\n",
    "    past = None\n",
    "    all_logits = []\n",
    "    target = []\n",
    "    total_loss = 0 \n",
    "    \n",
    "    for turn_num in range(len(batch_keep_indices)):\n",
    "\n",
    "        # data send to gpu\n",
    "        dialogs = batch_dialogs[turn_num]\n",
    "        dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "        user_tokens, user_pos, user_mask, \\\n",
    "            system_tokens, system_pos, system_mask, \\\n",
    "            belief_tokens, belief_pos, belief_mask = dialogs\n",
    "\n",
    "        # filtering algorithm\n",
    "        keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "        if len(keep_indices) != prev_batch_size:\n",
    "            past = filter_past(past, keep_indices)\n",
    "            mask = mask[keep_indices, :]\n",
    "\n",
    "        # User Utterance\n",
    "        mask = torch.cat([mask, user_mask], dim=-1)\n",
    "        logits, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "        all_logits.append(logits)\n",
    "        target.append(user_tokens)\n",
    "        # A_loss = calculate_loss(logits, user_tokens, user_mask)\n",
    "\n",
    "        # System Response\n",
    "        mask = torch.cat([mask, system_mask], dim=-1)\n",
    "        logits, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "        all_logits.append(logits)\n",
    "        target.append(system_tokens)\n",
    "        # B_loss = calculate_loss(logits, system_tokens, system_mask)\n",
    "\n",
    "        # tail\n",
    "        # total_loss = total_loss + user_weight * A_loss + B_loss\n",
    "        prev_batch_size = user_tokens.shape[0]\n",
    "\n",
    "#     breakpoint\n",
    "    all_logits = torch.cat(all_logits, dim=1)\n",
    "    all_logits = all_logits[:, :-1].contiguous()\n",
    "\n",
    "    target = torch.cat(target, dim=1)\n",
    "    target = target[:, 1:].contiguous()\n",
    "    \n",
    "    target_mask = torch.ones_like(target).float()\n",
    "    \n",
    "    total_loss = criterion(all_logits, target, target_mask, label_smoothing=0.02, reduce=True)\n",
    "\n",
    "    # gradient accumulation\n",
    "    total_loss /= len(batch_keep_indices)\n",
    "    total_loss /= num_gradients_accumulation \n",
    "    \n",
    "    if fp16:\n",
    "        with amp.scale_loss(total_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    else:\n",
    "        total_loss.backward()\n",
    "        \n",
    "    record_loss = total_loss.item() * num_gradients_accumulation\n",
    "    perplexity = np.exp(record_loss)\n",
    "    \n",
    "    return record_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, data):\n",
    "\n",
    "    model_A.eval()\n",
    "    model_B.eval()\n",
    "\n",
    "    temperature = 0.5\n",
    "\n",
    "    all_response = []\n",
    "\n",
    "    for batch_dialogs, batch_keep_indices in tqdm.tqdm_notebook(dataloader):\n",
    "\n",
    "        aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "        past = None\n",
    "        generated_responses = [[] for i in range(batch_dialogs[0][0].shape[0])]\n",
    "\n",
    "        mask = torch.ByteTensor([]).to(device)\n",
    "        prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for turn_num in range(len(batch_keep_indices)):\n",
    "                # data send to gpu\n",
    "                dialogs = batch_dialogs[turn_num]\n",
    "                dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "                user_tokens, user_pos, user_mask, \\\n",
    "                    system_tokens, system_pos, system_mask, \\\n",
    "                    belief_tokens, belief_pos, belief_mask = dialogs\n",
    "\n",
    "                # batch filtering algorithm\n",
    "                keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "                if len(keep_indices) != prev_batch_size:\n",
    "                    past = filter_past(past, keep_indices)\n",
    "                    mask = mask[keep_indices, :]\n",
    "\n",
    "                # define some initials\n",
    "                cur_batch_size = user_tokens.shape[0]\n",
    "                flags = np.ones(cur_batch_size)\n",
    "                generated_tokens = [[] for i in range(cur_batch_size)]\n",
    "\n",
    "                # feed in user\n",
    "                mask = torch.cat([mask, user_mask], dim=-1)\n",
    "                _, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "\n",
    "                # response generation\n",
    "                response = []\n",
    "\n",
    "\n",
    "                # first three tokens\n",
    "                prev_input = system_tokens[:, :3]\n",
    "                cur_pos = system_pos[:, :3]\n",
    "                temp_past = past\n",
    "                temp_mask = F.pad(mask, pad=(0,3), value=1)\n",
    "\n",
    "                # feed into B\n",
    "                logits, temp_past = model_B(prev_input, position_ids=cur_pos, mask=temp_mask, past=temp_past)\n",
    "                # set current position\n",
    "                cur_pos = cur_pos[:, -1].unsqueeze(1) + 1\n",
    "\n",
    "                for i in range(50):\n",
    "                    logits = logits[:, -1, :] / temperature\n",
    "                    prev_tokens = torch.argmax(logits, dim=-1)\n",
    "                    np_prev_tokens = prev_tokens.cpu().numpy()\n",
    "                    # nucleus sampling\n",
    "                    # logits = top_filtering(logits, top_k=100, top_p=0.7)\n",
    "                    # probs = F.softmax(logits, -1)\n",
    "                    # prev_input = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "                    # add to generated tokens list\n",
    "                    count = 0\n",
    "                    for idx, value in enumerate(flags):\n",
    "                        if value != 0:\n",
    "                            generated_tokens[idx].append(np_prev_tokens[count])\n",
    "                            count += 1\n",
    "\n",
    "                    # filtering algorithm\n",
    "                    if np.any(np_prev_tokens == 628):\n",
    "                        # set flags 0\n",
    "                        count = 0\n",
    "                        for idx, value in enumerate(flags):\n",
    "                            if value == 1:\n",
    "                                if np_prev_tokens[count] == 628:\n",
    "                                    flags[idx] = 0\n",
    "                                count += 1\n",
    "                        # compute which one to keep\n",
    "                        keep_indices = np.argwhere(np_prev_tokens != 628).squeeze(1)\n",
    "                        # filter\n",
    "                        prev_tokens = prev_tokens[keep_indices.tolist()]\n",
    "                        cur_pos = cur_pos[keep_indices.tolist(), :]\n",
    "                        temp_mask = temp_mask[keep_indices.tolist(), :]\n",
    "                        temp_past = [item[:, keep_indices.tolist()] for item in temp_past]\n",
    "                        np_prev_tokens = np_prev_tokens[keep_indices.tolist()]\n",
    "\n",
    "                    if np.all(flags == 0):\n",
    "                        break\n",
    "\n",
    "                    # prepare for the next token        \n",
    "                    temp_mask = F.pad(temp_mask, pad=(0, 1), value=1)\n",
    "                    logits, temp_past = model_B(prev_tokens.view(-1, 1), \n",
    "                                           position_ids=cur_pos, \n",
    "                                           mask=temp_mask, \n",
    "                                           past=temp_past)\n",
    "                    cur_pos = cur_pos + 1\n",
    "\n",
    "                # real system_tokens feed in\n",
    "                mask = torch.cat([mask, system_mask], dim=-1)\n",
    "                _, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "\n",
    "                # inject into generated_responses_list\n",
    "                decoded_responses = [tokenizer.decode(item).replace(\"\\n\", \"\") for item in generated_tokens]\n",
    "                count = 0\n",
    "                for idx in batch_keep_indices[turn_num]:\n",
    "                    generated_responses[idx].append(decoded_responses[count])\n",
    "                    count += 1\n",
    "\n",
    "            # add to the final responses        \n",
    "            for item in generated_responses:\n",
    "                all_response.extend(item)\n",
    "                \n",
    "    # Stage 2\n",
    "    #   prepare for metric eval\n",
    "    dialog_data = []\n",
    "    count = 0\n",
    "    all_results = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        raw_dialog = data[i]\n",
    "\n",
    "        for turn_num in range(len(raw_dialog)):\n",
    "\n",
    "            replaced_response = clean_sentence(\n",
    "                replace_punc(raw_dialog[turn_num][\"replaced_response\"].lower().replace(\"slot\", \"SLOT\")), entity_dict)\n",
    "\n",
    "            generated_response = clean_sentence(replace_punc(all_response[count].lower().replace(\"slot\", \"SLOT\")), entity_dict)\n",
    "\n",
    "            dialog_data.append({\"dial_id\": raw_dialog[turn_num][\"dial_id\"],\n",
    "                                \"turn_num\": raw_dialog[turn_num][\"turn_num\"],\n",
    "                                \"response\": replaced_response,\n",
    "                                \"generated_response\":generated_response \n",
    "                              })\n",
    "            count += 1\n",
    "            \n",
    "    sccuess_f1 = success_f1_metric(dialog_data)\n",
    "    bleu = bleu_metric(dialog_data)\n",
    "\n",
    "    return {\"bleu\": bleu,\n",
    "            \"sccuess_f1\": sccuess_f1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b763d4817c4807a1eb50d8769ba258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09f63293c2a46e6b115240adf11749b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.15030224079325677, 'sccuess_f1': 0.7289719576657404}\n",
      "Epoch 0 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b32bd17b4b4a9095ae9e115e0880b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.16065186514004107, 'sccuess_f1': 0.7229946474665906}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45af662eda240b28261b13e887d7edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063ccc342a8c4cc88932964c9e123857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.20159160456140635, 'sccuess_f1': 0.7890624949955751}\n",
      "Epoch 1 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f6c1733a58478db046b4b7db1b118b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.199001895901306, 'sccuess_f1': 0.7834008047204306}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc20b1c7b45b4dd180000b7825eb2e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07657f9868374c9f869aa8cd75c4a86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.21764169964334698, 'sccuess_f1': 0.7829534142309306}\n",
      "Epoch 2 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9c3fdfe9004d9a8dc0f2eb61bd1b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.2119331623490123, 'sccuess_f1': 0.8084677369362766}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dfb06c40ff4603a92b38e8180671ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002ad6cb12a84f82b21c6938952c556f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.17810661328456243, 'sccuess_f1': 0.8605504537011701}\n",
      "Epoch 3 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346513164f1e4bb887e7cbb7e66850b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.1917234592541125, 'sccuess_f1': 0.86223054793731}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d79199c3fe4dc59f3dba02dd9f25ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d024b9211048889cdcfd0bb3749aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.17735537818870867, 'sccuess_f1': 0.8124999949951172}\n",
      "Epoch 4 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd35e0a0a4c4d48b76f2a568eb5213e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.1661252896238959, 'sccuess_f1': 0.7781154965243609}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cd34b2460f4fbd835830b2b41e410d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10faeda15a094fbda7a51d936e6d9a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.21054865678571455, 'sccuess_f1': 0.850187260901752}\n",
      "Epoch 5 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec66229a62941b4850bd84767b18002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.21370389377744609, 'sccuess_f1': 0.8326923026767568}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e501eefa144dac975a11c871f5f30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e516c5ae8ec4e6a928b74f341fae486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.21756370208620154, 'sccuess_f1': 0.8527272677150083}\n",
      "Epoch 6 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcbdbc587a445978cbd4c93785ae6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.22080888819264005, 'sccuess_f1': 0.8643592092043043}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98008f85f78a4db5bf53a136cb326980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ecc7c0112b4bb081dae91268a37419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.22243687072446544, 'sccuess_f1': 0.8628884776187882}\n",
      "Epoch 7 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717b8237eade46acbb20774c8e4884fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.22013587116491093, 'sccuess_f1': 0.8617121304501981}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e48e0cda87249c78b31bb4284be9807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3ba38b60cd4ab49f4bc65ccb9c2bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.22513808145971756, 'sccuess_f1': 0.8464419425497623}\n",
      "Epoch 8 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f3878225fb4dc18ba17b1e40b4e0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.21622798492409412, 'sccuess_f1': 0.8522072886499644}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c80a7c4880443391c1229e4a96d91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9784aeb13341128fd8e425582eba24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.21705524420698574, 'sccuess_f1': 0.8512628574724198}\n",
      "Epoch 9 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1692b241b9644139bd7589919b84d21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.2237937485513571, 'sccuess_f1': 0.8444444394291769}\n"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "progress_bar = tqdm.tqdm_notebook\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    \"Training\"\n",
    "    pbar = progress_bar(train_dataloader)\n",
    "    model_A.train()\n",
    "    model_B.train()\n",
    "    \n",
    "    for batch_dialogs, batch_keep_indices in pbar:\n",
    "        \n",
    "        record_loss, perplexity = train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False)\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:\n",
    "            # update for gradient accumulation\n",
    "            scheduler.step()\n",
    "#             torch.nn.utils.clip_grad_norm_(model_A.parameters(), 5.0)\n",
    "#             torch.nn.utils.clip_grad_norm_(model_B.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # speed measure\n",
    "            end = time.time()\n",
    "            speed = train_batch_size * num_gradients_accumulation / (end - start)\n",
    "            start = end\n",
    "            \n",
    "            # show progress\n",
    "            pbar.set_postfix(loss=record_loss, perplexity=perplexity, speed=speed)\n",
    "    \n",
    "    \"Evaluation\"\n",
    "    print(f\"Epoch {ep} Validation\")\n",
    "    eval_res = validate(val_dataloader, val_data)\n",
    "    print(eval_res)\n",
    "    \n",
    "    print(f\"Epoch {ep} Test\")\n",
    "    eval_res = validate(test_dataloader, test_data)\n",
    "    print(eval_res)\n",
    "    \n",
    "    checkpointer.save_checkpoint(ep, \n",
    "                                 [model_A.state_dict(), model_A.state_dict()],\n",
    "                                 {\"None\": None},\n",
    "                                 True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
