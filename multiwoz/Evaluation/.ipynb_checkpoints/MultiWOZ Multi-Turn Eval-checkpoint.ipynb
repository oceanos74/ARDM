{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchfly\n",
    "torchfly.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from apex import amp\n",
    "from allennlp.training.checkpointer import Checkpointer\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule, GPT2Tokenizer\n",
    "\n",
    "from torchfly.criterions import SequenceFocalLoss, SequenceCrossEntropyLoss\n",
    "from torchfly.decode import top_filtering\n",
    "from gpt_model import GPT2SimpleLM\n",
    "from text_utils import recoverText, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.sep_token = \"None\"\n",
    "# add speicial tokens in the same order as Roberta\n",
    "tokenizer.add_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = GPT2SimpleLM(GPT2SmallConfig)\n",
    "model_B = GPT2SimpleLM(GPT2SmallConfig)\n",
    "model_A_states, model_B_states = torch.load(\"../Checkpoint (copy)/model_state_epoch_5.th\")\n",
    "model_A.load_state_dict(model_A_states)\n",
    "model_B.load_state_dict(model_B_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_keep_indices(batch_keep_indices):\n",
    "    prev = batch_keep_indices[1]\n",
    "    new_batch_keep_indices = [prev]\n",
    "\n",
    "    for i in range(1, len(batch_keep_indices)):\n",
    "        curr = batch_keep_indices[i]\n",
    "        new = []\n",
    "\n",
    "        for idx in curr:\n",
    "            new.append(prev.index(idx))\n",
    "\n",
    "        new_batch_keep_indices.append(new)\n",
    "        prev = curr\n",
    "        \n",
    "    return new_batch_keep_indices\n",
    "\n",
    "\n",
    "class MultiWOZDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.file_list = list(self.data.keys())\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bos = tokenizer.encode(\"<s>\")\n",
    "        self.user_bos = tokenizer.encode(\"A:\")\n",
    "        self.system_bos = tokenizer.encode(\"B:\")\n",
    "        \n",
    "        self.eos = [628, 198]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_list[index]\n",
    "        full_dialog = self.data[file_name]['log']\n",
    "        \n",
    "        full_dialog_tokens = []\n",
    "        cur_pos = 0\n",
    "        \n",
    "        for turn_dialog in full_dialog:\n",
    "            # cur_pos = 0\n",
    "            \n",
    "            # user\n",
    "            user = recoverText(turn_dialog['user_delex'])\n",
    "            # user = recoverText(turn_dialog['user_delex'])\n",
    "            user_tokens = self.user_bos + tokenizer.encode(user) + self.eos\n",
    "\n",
    "            user_pos = torch.arange(cur_pos, cur_pos + len(user_tokens))\n",
    "            cur_pos = user_pos[-1] + 1\n",
    "            \n",
    "            # belief span\n",
    "#             belief_tokens = self.bos + \\\n",
    "#                             tokenizer.encode(\";\".join(turn_dialog['bspan_inform'][1:])) + \\\n",
    "#                             self.eos\n",
    "#             belief_pos = torch.arange(cur_pos, cur_pos + len(belief_tokens))\n",
    "#             cur_pos = belief_pos[-1]\n",
    "\n",
    "\n",
    "            # Database\n",
    "            if eval(turn_dialog['pointer'])[-2:] == (1, 0):\n",
    "                booked = \"book\"\n",
    "            elif eval(turn_dialog['pointer'])[-2:] == (0, 1):\n",
    "                booked = \"fail\"\n",
    "            else:\n",
    "                booked = \"none\"\n",
    "            \n",
    "            if len(turn_dialog['match']) > 0:\n",
    "                num_match = int(turn_dialog['match']) if int(turn_dialog['match']) < 4 else 4\n",
    "            else:\n",
    "                num_match = 0\n",
    "                \n",
    "            database = str(num_match) + \";\" + booked + \";\" + turn_dialog['turn_domain'].strip(\"[]\") + \";\"\n",
    "            database_tokens = tokenizer.encode(database)\n",
    "            database_pos = torch.arange(cur_pos, cur_pos + len(database_tokens))\n",
    "            cur_pos = database_pos[-1] + 1\n",
    "            \n",
    "            # System\n",
    "            system = recoverText(process_text(turn_dialog['resp'], turn_dialog['turn_domain'].strip(\"[]\")))\n",
    "            system_tokens = self.system_bos + tokenizer.encode(system) + self.eos\n",
    "            system_pos = torch.arange(cur_pos, cur_pos + len(system_tokens))\n",
    "            cur_pos = system_pos[-1] + 1\n",
    "            \n",
    "            user_tokens = torch.LongTensor(user_tokens)\n",
    "            system_tokens = torch.LongTensor(system_tokens)\n",
    "            database_tokens = torch.LongTensor(database_tokens)\n",
    "            \n",
    "            full_dialog_tokens.append((user_tokens, \n",
    "                                       user_pos,\n",
    "                                       system_tokens, \n",
    "                                       system_pos,\n",
    "                                       database_tokens,\n",
    "                                       database_pos))\n",
    "#             if system_pos[-1] > 1:\n",
    "#                 break\n",
    "\n",
    "        return full_dialog_tokens, file_name\n",
    "\n",
    "\n",
    "def calculate_length(dialogs):\n",
    "    total_sum = 0\n",
    "    for turn_num in range(len(dialogs)):\n",
    "        total_sum += len(dialogs[turn_num][1]) + \\\n",
    "                        len(dialogs[turn_num][3]) + \\\n",
    "                        len(dialogs[turn_num][5])        \n",
    "    return total_sum\n",
    "\n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = self.tokenizer.encode(\"<pad>\")[0]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "        unpacked_data, file_names = zip(*unpacked_data)\n",
    "        \n",
    "        keep_indices = []\n",
    "        for i, dialog in enumerate(unpacked_data):\n",
    "            length = calculate_length(dialog)\n",
    "            if length < 900:\n",
    "                keep_indices.append(i)\n",
    "        \n",
    "        unpacked_data = [unpacked_data[idx] for idx in keep_indices]\n",
    "        file_names = [file_names[idx] for idx in keep_indices]\n",
    "    \n",
    "        if len(unpacked_data) == 0:\n",
    "            return None, None, None\n",
    "        \n",
    "        max_turn_len = max([len(item) for item in unpacked_data])\n",
    "    \n",
    "        batch_dialogs = []\n",
    "        batch_keep_indices = []\n",
    "\n",
    "        for turn_num in range(max_turn_len):\n",
    "\n",
    "            keep_indices = []\n",
    "\n",
    "            for batch_idx in range(len(unpacked_data)):\n",
    "                if turn_num < len(unpacked_data[batch_idx]):\n",
    "                    keep_indices.append(batch_idx)\n",
    "\n",
    "            user_tokens = pad_sequence([unpacked_data[idx][turn_num][0] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            user_pos = pad_sequence([unpacked_data[idx][turn_num][1] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            system_tokens = pad_sequence([unpacked_data[idx][turn_num][2] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            system_pos = pad_sequence([unpacked_data[idx][turn_num][3] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            database_tokens = pad_sequence([unpacked_data[idx][turn_num][4] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            database_pos = pad_sequence([unpacked_data[idx][turn_num][5] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)  \n",
    "\n",
    "            user_mask = (user_tokens != self.pad).byte()\n",
    "            system_mask = (system_tokens != self.pad).byte()\n",
    "            database_mask = (database_tokens != self.pad).byte()\n",
    "\n",
    "\n",
    "            batch_dialogs.append((user_tokens, user_pos, user_mask, \n",
    "                                  system_tokens, system_pos, system_mask, \n",
    "                                  database_tokens, database_pos, database_mask))\n",
    "            batch_keep_indices.append(keep_indices)\n",
    "            \n",
    "        return batch_dialogs, batch_keep_indices, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_past(past, keep_indices):\n",
    "    past = [item[:, keep_indices] for item in past]\n",
    "    return past\n",
    "\n",
    "def replace_punc(x):\n",
    "    x = x.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    return x.replace(\".\", \" .\").replace(\",\", \" .\").replace(\"?\", \" ?\").replace(\"?\", \" ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../yichi_data/val_data_dict.json\") as f:\n",
    "    val_data = json.load(f)\n",
    "    \n",
    "with open(\"../../yichi_data/test_data_dict.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "val_dataset = MultiWOZDataset(val_data, tokenizer)\n",
    "test_dataset = MultiWOZDataset(test_data, tokenizer)\n",
    "\n",
    "eval_batch_size = 16\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_B.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, domain):\n",
    "    text = text.replace(\"[value_choice]\", \"[value_count]\")\n",
    "    text = text.replace(\"[value_people]\", \"[value_count]\")\n",
    "    text = text.replace(\"[value_starts]\", \"[value_count]\")\n",
    "    \n",
    "    text = text.replace(\"[value_car]\", '[taxi_type]')\n",
    "    text = text.replace(\"[value_leave]\", \"[value_time]\")\n",
    "    text = text.replace(\"[value_arrive]\", \"[value_time]\")\n",
    "    text = text.replace(\"[value_price]\", \"[value_pricerange]\")\n",
    "\n",
    "    text = text.replace('[value_postcode]', f'[{domain}_postcode]')\n",
    "    text = text.replace('[value_reference]', f'[{domain}_reference]')\n",
    "    text = text.replace('[value_address]', f'[{domain}_address]')\n",
    "    text = text.replace('[value_phone]', f'[{domain}_phone]')\n",
    "    text = text.replace('[value_name]', f'[{domain}_name]')\n",
    "    text = text.replace('[value_id]', f'[{domain}_id]')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_dialogs, batch_keep_indices):\n",
    "    \n",
    "    aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "    past = None\n",
    "    generated_responses = [[] for i in range(batch_dialogs[0][0].shape[0])]\n",
    "\n",
    "    mask = torch.ByteTensor([]).to(device)\n",
    "    prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for turn_num in range(len(batch_keep_indices)):\n",
    "            # data send to gpu\n",
    "            dialogs = batch_dialogs[turn_num]\n",
    "            dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "            user_tokens, user_pos, user_mask, \\\n",
    "                system_tokens, system_pos, system_mask, \\\n",
    "                database_tokens, database_pos, database_mask = dialogs\n",
    "\n",
    "            # batch filtering algorithm\n",
    "            keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "            if len(keep_indices) != prev_batch_size:\n",
    "                past = filter_past(past, keep_indices)\n",
    "                mask = mask[keep_indices, :]\n",
    "\n",
    "            # define some initials\n",
    "            cur_batch_size = user_tokens.shape[0]\n",
    "            flags = np.ones(cur_batch_size)\n",
    "            generated_tokens = [[] for i in range(cur_batch_size)]\n",
    "\n",
    "            # feed in user\n",
    "            mask = torch.cat([mask, user_mask], dim=-1)\n",
    "            _, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "\n",
    "            # response generation\n",
    "            response = []\n",
    "\n",
    "            # database tokens\n",
    "            mask = torch.cat([mask, database_mask], dim=-1)\n",
    "            _, past = model_B(database_tokens, position_ids=database_pos, mask=mask, past=past)\n",
    "\n",
    "            # response generation\n",
    "            prev_input = system_tokens[:, :2]\n",
    "            cur_pos = system_pos[:, :2]\n",
    "            temp_past = past\n",
    "            temp_mask = F.pad(mask, pad=(0,2), value=1)\n",
    "\n",
    "            # feed into B\n",
    "            logits, temp_past = model_B(prev_input, position_ids=cur_pos, mask=temp_mask, past=temp_past)\n",
    "            # set current position\n",
    "            cur_pos = cur_pos[:, -1].unsqueeze(1) + 1\n",
    "\n",
    "\n",
    "            for i in range(60):\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                # logits = top_filtering(logits, top_p=0.2)\n",
    "                # probs = F.softmax(logits, -1)\n",
    "                # prev_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "                prev_tokens = torch.argmax(logits, dim=-1)\n",
    "                np_prev_tokens = prev_tokens.cpu().numpy()\n",
    "                # nucleus sampling\n",
    "\n",
    "\n",
    "                # add to generated tokens list\n",
    "                count = 0\n",
    "                for idx, value in enumerate(flags):\n",
    "                    if value != 0:\n",
    "                        generated_tokens[idx].append(np_prev_tokens[count])\n",
    "                        count += 1\n",
    "\n",
    "                # filtering algorithm\n",
    "                if np.any(np_prev_tokens == 628):\n",
    "                    # set flags 0\n",
    "                    count = 0\n",
    "                    for idx, value in enumerate(flags):\n",
    "                        if value == 1:\n",
    "                            if np_prev_tokens[count] == 628:\n",
    "                                flags[idx] = 0\n",
    "                            count += 1\n",
    "                    # compute which one to keep\n",
    "                    keep_indices = np.argwhere(np_prev_tokens != 628).squeeze(1)\n",
    "                    # filter\n",
    "                    prev_tokens = prev_tokens[keep_indices.tolist()]\n",
    "                    cur_pos = cur_pos[keep_indices.tolist(), :]\n",
    "                    temp_mask = temp_mask[keep_indices.tolist(), :]\n",
    "                    temp_past = [item[:, keep_indices.tolist()] for item in temp_past]\n",
    "                    np_prev_tokens = np_prev_tokens[keep_indices.tolist()]\n",
    "\n",
    "                if np.all(flags == 0):\n",
    "                    break\n",
    "\n",
    "                # prepare for the next token        \n",
    "                temp_mask = F.pad(temp_mask, pad=(0, 1), value=1)\n",
    "                logits, temp_past = model_B(prev_tokens.view(-1, 1), \n",
    "                                       position_ids=cur_pos, \n",
    "                                       mask=temp_mask, \n",
    "                                       past=temp_past)\n",
    "                cur_pos = cur_pos + 1\n",
    "\n",
    "            # real system_tokens feed in\n",
    "            mask = torch.cat([mask, system_mask], dim=-1)\n",
    "            _, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "\n",
    "            # inject into generated_responses_list\n",
    "            decoded_responses = [tokenizer.decode(item).replace(\"\\n\", \"\") for item in generated_tokens]\n",
    "            count = 0\n",
    "            for idx in batch_keep_indices[turn_num]:\n",
    "                generated_responses[idx].append(decoded_responses[count])\n",
    "                count += 1\n",
    "                \n",
    "    return generated_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accaf9ff315141e6bbec43596f87c493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_test_pred = {}\n",
    "\n",
    "for batch_dialogs, batch_keep_indices, file_names in tqdm.tqdm_notebook(test_dataloader):\n",
    "    if batch_dialogs is None:\n",
    "        continue\n",
    "    \n",
    "    generated_responses = generate(batch_dialogs, batch_keep_indices)\n",
    "    for i, pred_dialog in enumerate(generated_responses):\n",
    "        all_test_pred[file_names[i]] = pred_dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latent_dialog.evaluators import MultiWozEvaluator, BLEUScorer\n",
    "from text_utils import recoverText, normalize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"all_test_pred.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(all_test_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"all_test_pred.pkl\", \"rb\") as f:\n",
    "#     all_test_pred = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_name = \"test\"\n",
    "evaluator = MultiWozEvaluator(data_name)\n",
    "\n",
    "evaluator.initialize()\n",
    "eval_data = {}\n",
    "\n",
    "for file_name in all_test_pred:\n",
    "    eval_data[file_name] = [normalize(item) for item in all_test_pred[file_name]]\n",
    "    \n",
    "    for i, sentence in enumerate(eval_data[file_name]):\n",
    "        evaluator.add_example(sentence, sentence)\n",
    "        \n",
    "report, successes, matches, failure_files = evaluator.evaluateModel(eval_data, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Corpus Matches : 84.78%\n",
      "test Corpus Success : 70.87%\n",
      "Total number of dialogues: 999 \n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failure_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usr:i am looking for information in cambridge\n",
      "Sys:ok sure . what would you like to know ?\n",
      "Usr:i have an upcoming conference in cambridge and need to figure out transportation . can you tell me about a train to take maybe ?\n",
      "Sys:absolutely . where are you heading in from ? what day ?\n",
      "Usr:i 'll be leaving london kings cross and heading to cambridge . i need to be there by 10:30 on tuesday . can you book this for 3 people ? reference please ?\n",
      "Sys:i have a train arriving at [value_arrive] . would that do ?\n",
      "Usr:yes . book for 3 people .\n",
      "Sys:alright got you booked on train [value_id] , the total fee is [value_price] payable at the station . your reference number is : [value_reference] . anything else i can help with today ?\n",
      "Usr:yes i am looking for someplace to go in the south for entertainment .\n",
      "Sys:we have [value_choice] options , can i reccomend for you ?\n",
      "Usr:which ever is nicer . i will need some info on it too .\n",
      "Sys:i recommend [value_name] it is in [value_address] postcode [value_postcode] and their number is [value_phone] . is there anything else i can help with today ?\n",
      "Usr:no , that is all i needed today . thanks for your help , it is much appreciated .\n",
      "Sys:thank you for using our service !\n"
     ]
    }
   ],
   "source": [
    "filename = 'PMUL3672.json'\n",
    "\n",
    "for i in range(len(test_data[filename]['log'])):\n",
    "    print(\"Usr:\" + test_data[filename]['log'][i]['user'])\n",
    "    print(\"Sys:\" + test_data[filename]['log'][i]['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usr: where is a 4 star hotel located in north cambridge ?\n",
      "Sys: there are [value_count] attractions in cambridge . do you have a specific area in mind ?\n",
      "Usr: sure , that could be nice\n",
      "Sys: [attraction_name] is located in the [value_area] and is [value_pricerange] .\n",
      "Usr: i actually do n't need reservations i just need the phone number , price range .\n",
      "Sys: [attraction_name] is [value_pricerange] to enter .\n",
      "Usr: okay . now could you help me find a restaurant in the expensive price range that is in the same area as the hotel ?\n",
      "Sys: [train_id] leaves at [value_time] and arrives by [value_time] . would you like me to book it for you ?\n",
      "Usr: before we do that , what is the name of the guest house ? and also , do they have free parking ?\n",
      "Sys: your booking was successful . the reference number is [train_reference] .\n",
      "Usr: could you recommend an expensive restaurant in the same area ?\n",
      "Sys: you are welcome . have a great day !\n",
      "Usr: yes , book me a table for 2 people at 12:15 on monday .\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-93ad75918ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Usr: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sys: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "filename = 'PMUL0079.json'\n",
    "\n",
    "for i in range(len(test_data[filename]['log'])):\n",
    "    print(\"Usr: \" + test_data[filename]['log'][i]['user'])\n",
    "    print(\"Sys: \" + eval_data[file_name][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data[filename]['log']))\n",
    "len(eval_data[file_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames, _ = zip(*failure_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data['PMUL2859.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/norm-multi-woz/test_dials.json\") as f:\n",
    "    gt_test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item['resp'] for item in test_data['PMUL2859.json']['log']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item['user'] for item in test_data['PMUL2859.json']['log']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item['turn_domain'] for item in test_data['PMUL2859.json']['log']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['PMUL2859.json']['sys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluateModel(eval_data, mode='rollout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_test_pred = {}\n",
    "\n",
    "for file_name in test_data:\n",
    "    rollout_test_pred[file_name] = delex_data[file_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_test_pred = {}\n",
    "\n",
    "for file_name in test_data:\n",
    "    generated_test_pred[file_name] = [item['text'].strip() \n",
    "                                      for i, item in enumerate(delex_data[file_name]['log']) \n",
    "                                      if i % 2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"rollout\"\n",
    "evaluator = MultiWozEvaluator(data_name)\n",
    "\n",
    "evaluator.initialize()\n",
    "eval_data = {}\n",
    "\n",
    "evaluator.evaluateModel(rollout_test_pred, mode='rollout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"test\"\n",
    "evaluator = MultiWozEvaluator(data_name)\n",
    "\n",
    "evaluator.initialize()\n",
    "eval_data = {}\n",
    "\n",
    "evaluator.evaluateModel(generated_test_pred, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
