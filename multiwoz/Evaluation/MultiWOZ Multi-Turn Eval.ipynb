{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchfly\n",
    "torchfly.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from apex import amp\n",
    "from allennlp.training.checkpointer import Checkpointer\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule, GPT2Tokenizer\n",
    "\n",
    "from torchfly.criterions import SequenceFocalLoss, SequenceCrossEntropyLoss\n",
    "from torchfly.decode import top_filtering\n",
    "from gpt_model import GPT2SimpleLM\n",
    "from text_utils import recoverText, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.sep_token = \"None\"\n",
    "# add speicial tokens in the same order as Roberta\n",
    "tokenizer.add_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = GPT2SimpleLM(GPT2SmallConfig)\n",
    "model_B = GPT2SimpleLM(GPT2SmallConfig)\n",
    "model_A_states, model_B_states = torch.load(\"../Checkpoint/model_state_epoch_5.th\")\n",
    "model_A.load_state_dict(model_A_states)\n",
    "model_B.load_state_dict(model_B_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_keep_indices(batch_keep_indices):\n",
    "    prev = batch_keep_indices[1]\n",
    "    new_batch_keep_indices = [prev]\n",
    "\n",
    "    for i in range(1, len(batch_keep_indices)):\n",
    "        curr = batch_keep_indices[i]\n",
    "        new = []\n",
    "\n",
    "        for idx in curr:\n",
    "            new.append(prev.index(idx))\n",
    "\n",
    "        new_batch_keep_indices.append(new)\n",
    "        prev = curr\n",
    "        \n",
    "    return new_batch_keep_indices\n",
    "\n",
    "\n",
    "class MultiWOZDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.file_list = list(self.data.keys())\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bos = tokenizer.encode(\"<s>\")\n",
    "        self.user_bos = tokenizer.encode(\"A:\")\n",
    "        self.system_bos = tokenizer.encode(\"B:\")\n",
    "        \n",
    "        self.eos = [628, 198]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_list[index]\n",
    "        full_dialog = self.data[file_name]['log']\n",
    "        \n",
    "        full_dialog_tokens = []\n",
    "        cur_pos = 0\n",
    "        \n",
    "        for turn_dialog in full_dialog:\n",
    "            # cur_pos = 0\n",
    "            \n",
    "            # user\n",
    "            user = recoverText(turn_dialog['user_delex'])\n",
    "            # user = recoverText(turn_dialog['user_delex'])\n",
    "            user_tokens = self.user_bos + tokenizer.encode(user) + self.eos\n",
    "\n",
    "            user_pos = torch.arange(cur_pos, cur_pos + len(user_tokens))\n",
    "            cur_pos = user_pos[-1] + 1\n",
    "            \n",
    "            # belief span\n",
    "#             belief_tokens = self.bos + \\\n",
    "#                             tokenizer.encode(\";\".join(turn_dialog['bspan_inform'][1:])) + \\\n",
    "#                             self.eos\n",
    "#             belief_pos = torch.arange(cur_pos, cur_pos + len(belief_tokens))\n",
    "#             cur_pos = belief_pos[-1]\n",
    "\n",
    "\n",
    "            # Database\n",
    "            if eval(turn_dialog['pointer'])[-2:] == (1, 0):\n",
    "                booked = \"book\"\n",
    "            elif eval(turn_dialog['pointer'])[-2:] == (0, 1):\n",
    "                booked = \"fail\"\n",
    "            else:\n",
    "                booked = \"none\"\n",
    "            \n",
    "            if len(turn_dialog['match']) > 0:\n",
    "                num_match = int(turn_dialog['match']) if int(turn_dialog['match']) < 4 else 4\n",
    "            else:\n",
    "                num_match = 0\n",
    "                \n",
    "            database = str(num_match) + \";\" + booked + \";\" + turn_dialog['turn_domain'].strip(\"[]\") + \";\"\n",
    "            database_tokens = tokenizer.encode(database)\n",
    "            database_pos = torch.arange(cur_pos, cur_pos + len(database_tokens))\n",
    "            cur_pos = database_pos[-1] + 1\n",
    "            \n",
    "            # System\n",
    "            system = recoverText(process_text(turn_dialog['resp'], turn_dialog['turn_domain'].strip(\"[]\")))\n",
    "            system_tokens = self.system_bos + tokenizer.encode(system) + self.eos\n",
    "            system_pos = torch.arange(cur_pos, cur_pos + len(system_tokens))\n",
    "            cur_pos = system_pos[-1] + 1\n",
    "            \n",
    "            user_tokens = torch.LongTensor(user_tokens)\n",
    "            system_tokens = torch.LongTensor(system_tokens)\n",
    "            database_tokens = torch.LongTensor(database_tokens)\n",
    "            \n",
    "            full_dialog_tokens.append((user_tokens, \n",
    "                                       user_pos,\n",
    "                                       system_tokens, \n",
    "                                       system_pos,\n",
    "                                       database_tokens,\n",
    "                                       database_pos))\n",
    "#             if system_pos[-1] > 1:\n",
    "#                 break\n",
    "\n",
    "        return full_dialog_tokens, file_name\n",
    "\n",
    "\n",
    "def calculate_length(dialogs):\n",
    "    total_sum = 0\n",
    "    for turn_num in range(len(dialogs)):\n",
    "        total_sum += len(dialogs[turn_num][1]) + \\\n",
    "                        len(dialogs[turn_num][3]) + \\\n",
    "                        len(dialogs[turn_num][5])        \n",
    "    return total_sum\n",
    "\n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = self.tokenizer.encode(\"<pad>\")[0]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "        unpacked_data, file_names = zip(*unpacked_data)\n",
    "        \n",
    "        keep_indices = []\n",
    "        for i, dialog in enumerate(unpacked_data):\n",
    "            length = calculate_length(dialog)\n",
    "            if length < 900:\n",
    "                keep_indices.append(i)\n",
    "        \n",
    "        unpacked_data = [unpacked_data[idx] for idx in keep_indices]\n",
    "        file_names = [file_names[idx] for idx in keep_indices]\n",
    "    \n",
    "        if len(unpacked_data) == 0:\n",
    "            return None, None, None\n",
    "        \n",
    "        max_turn_len = max([len(item) for item in unpacked_data])\n",
    "    \n",
    "        batch_dialogs = []\n",
    "        batch_keep_indices = []\n",
    "\n",
    "        for turn_num in range(max_turn_len):\n",
    "\n",
    "            keep_indices = []\n",
    "\n",
    "            for batch_idx in range(len(unpacked_data)):\n",
    "                if turn_num < len(unpacked_data[batch_idx]):\n",
    "                    keep_indices.append(batch_idx)\n",
    "\n",
    "            user_tokens = pad_sequence([unpacked_data[idx][turn_num][0] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            user_pos = pad_sequence([unpacked_data[idx][turn_num][1] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            system_tokens = pad_sequence([unpacked_data[idx][turn_num][2] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            system_pos = pad_sequence([unpacked_data[idx][turn_num][3] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            database_tokens = pad_sequence([unpacked_data[idx][turn_num][4] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            database_pos = pad_sequence([unpacked_data[idx][turn_num][5] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)  \n",
    "\n",
    "            user_mask = (user_tokens != self.pad).byte()\n",
    "            system_mask = (system_tokens != self.pad).byte()\n",
    "            database_mask = (database_tokens != self.pad).byte()\n",
    "\n",
    "\n",
    "            batch_dialogs.append((user_tokens, user_pos, user_mask, \n",
    "                                  system_tokens, system_pos, system_mask, \n",
    "                                  database_tokens, database_pos, database_mask))\n",
    "            batch_keep_indices.append(keep_indices)\n",
    "            \n",
    "        return batch_dialogs, batch_keep_indices, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_past(past, keep_indices):\n",
    "    past = [item[:, keep_indices] for item in past]\n",
    "    return past\n",
    "\n",
    "def replace_punc(x):\n",
    "    x = x.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    return x.replace(\".\", \" .\").replace(\",\", \" .\").replace(\"?\", \" ?\").replace(\"?\", \" ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../yichi_data/val_data_dict.json\") as f:\n",
    "    val_data = json.load(f)\n",
    "    \n",
    "with open(\"../../yichi_data/test_data_dict.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "val_dataset = MultiWOZDataset(val_data, tokenizer)\n",
    "test_dataset = MultiWOZDataset(test_data, tokenizer)\n",
    "\n",
    "eval_batch_size = 16\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_B.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, domain):\n",
    "    text = text.replace(\"[value_choice]\", \"[value_count]\")\n",
    "    text = text.replace(\"[value_people]\", \"[value_count]\")\n",
    "    text = text.replace(\"[value_starts]\", \"[value_count]\")\n",
    "    \n",
    "    text = text.replace(\"[value_car]\", '[taxi_type]')\n",
    "    text = text.replace(\"[value_leave]\", \"[value_time]\")\n",
    "    text = text.replace(\"[value_arrive]\", \"[value_time]\")\n",
    "    text = text.replace(\"[value_price]\", \"[value_pricerange]\")\n",
    "\n",
    "    text = text.replace('[value_postcode]', f'[{domain}_postcode]')\n",
    "    text = text.replace('[value_reference]', f'[{domain}_reference]')\n",
    "    text = text.replace('[value_address]', f'[{domain}_address]')\n",
    "    text = text.replace('[value_phone]', f'[{domain}_phone]')\n",
    "    text = text.replace('[value_name]', f'[{domain}_name]')\n",
    "    text = text.replace('[value_id]', f'[{domain}_id]')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_dialogs, batch_keep_indices):\n",
    "    \n",
    "    aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "    past = None\n",
    "    generated_responses = [[] for i in range(batch_dialogs[0][0].shape[0])]\n",
    "\n",
    "    mask = torch.ByteTensor([]).to(device)\n",
    "    prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for turn_num in range(len(batch_keep_indices)):\n",
    "            # data send to gpu\n",
    "            dialogs = batch_dialogs[turn_num]\n",
    "            dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "            user_tokens, user_pos, user_mask, \\\n",
    "                system_tokens, system_pos, system_mask, \\\n",
    "                database_tokens, database_pos, database_mask = dialogs\n",
    "\n",
    "            # batch filtering algorithm\n",
    "            keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "            if len(keep_indices) != prev_batch_size:\n",
    "                past = filter_past(past, keep_indices)\n",
    "                mask = mask[keep_indices, :]\n",
    "\n",
    "            # define some initials\n",
    "            cur_batch_size = user_tokens.shape[0]\n",
    "            flags = np.ones(cur_batch_size)\n",
    "            generated_tokens = [[] for i in range(cur_batch_size)]\n",
    "\n",
    "            # feed in user\n",
    "            mask = torch.cat([mask, user_mask], dim=-1)\n",
    "            _, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "\n",
    "            # response generation\n",
    "            response = []\n",
    "\n",
    "            # database tokens\n",
    "            mask = torch.cat([mask, database_mask], dim=-1)\n",
    "            _, past = model_B(database_tokens, position_ids=database_pos, mask=mask, past=past)\n",
    "\n",
    "            # response generation\n",
    "            prev_input = system_tokens[:, :2]\n",
    "            cur_pos = system_pos[:, :2]\n",
    "            temp_past = past\n",
    "            temp_mask = F.pad(mask, pad=(0,2), value=1)\n",
    "\n",
    "            # feed into B\n",
    "            logits, temp_past = model_B(prev_input, position_ids=cur_pos, mask=temp_mask, past=temp_past)\n",
    "            # set current position\n",
    "            cur_pos = cur_pos[:, -1].unsqueeze(1) + 1\n",
    "\n",
    "\n",
    "            for i in range(60):\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                logits = top_filtering(logits, top_p=0.2)\n",
    "                probs = F.softmax(logits, -1)\n",
    "                prev_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "                # prev_tokens = torch.argmax(logits, dim=-1)\n",
    "                np_prev_tokens = prev_tokens.cpu().numpy()\n",
    "                # nucleus sampling\n",
    "\n",
    "\n",
    "                # add to generated tokens list\n",
    "                count = 0\n",
    "                for idx, value in enumerate(flags):\n",
    "                    if value != 0:\n",
    "                        generated_tokens[idx].append(np_prev_tokens[count])\n",
    "                        count += 1\n",
    "\n",
    "                # filtering algorithm\n",
    "                if np.any(np_prev_tokens == 628):\n",
    "                    # set flags 0\n",
    "                    count = 0\n",
    "                    for idx, value in enumerate(flags):\n",
    "                        if value == 1:\n",
    "                            if np_prev_tokens[count] == 628:\n",
    "                                flags[idx] = 0\n",
    "                            count += 1\n",
    "                    # compute which one to keep\n",
    "                    keep_indices = np.argwhere(np_prev_tokens != 628).squeeze(1)\n",
    "                    # filter\n",
    "                    prev_tokens = prev_tokens[keep_indices.tolist()]\n",
    "                    cur_pos = cur_pos[keep_indices.tolist(), :]\n",
    "                    temp_mask = temp_mask[keep_indices.tolist(), :]\n",
    "                    temp_past = [item[:, keep_indices.tolist()] for item in temp_past]\n",
    "                    np_prev_tokens = np_prev_tokens[keep_indices.tolist()]\n",
    "\n",
    "                if np.all(flags == 0):\n",
    "                    break\n",
    "\n",
    "                # prepare for the next token        \n",
    "                temp_mask = F.pad(temp_mask, pad=(0, 1), value=1)\n",
    "                logits, temp_past = model_B(prev_tokens.view(-1, 1), \n",
    "                                       position_ids=cur_pos, \n",
    "                                       mask=temp_mask, \n",
    "                                       past=temp_past)\n",
    "                cur_pos = cur_pos + 1\n",
    "\n",
    "            # real system_tokens feed in\n",
    "            mask = torch.cat([mask, system_mask], dim=-1)\n",
    "            _, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "\n",
    "            # inject into generated_responses_list\n",
    "            decoded_responses = [tokenizer.decode(item).replace(\"\\n\", \"\") for item in generated_tokens]\n",
    "            count = 0\n",
    "            for idx in batch_keep_indices[turn_num]:\n",
    "                generated_responses[idx].append(decoded_responses[count])\n",
    "                count += 1\n",
    "                \n",
    "    return generated_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accaf9ff315141e6bbec43596f87c493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_test_pred = {}\n",
    "\n",
    "for batch_dialogs, batch_keep_indices, file_names in tqdm.tqdm_notebook(test_dataloader):\n",
    "    if batch_dialogs is None:\n",
    "        continue\n",
    "    \n",
    "    generated_responses = generate(batch_dialogs, batch_keep_indices)\n",
    "    for i, pred_dialog in enumerate(generated_responses):\n",
    "        all_test_pred[file_names[i]] = pred_dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latent_dialog.evaluators import MultiWozEvaluator, BLEUScorer\n",
    "from text_utils import recoverText, normalize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"all_test_pred.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(all_test_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"all_test_pred.pkl\", \"rb\") as f:\n",
    "#     all_test_pred = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_name = \"test\"\n",
    "evaluator = MultiWozEvaluator(data_name)\n",
    "\n",
    "evaluator.initialize()\n",
    "eval_data = {}\n",
    "\n",
    "for file_name in all_test_pred:\n",
    "    eval_data[file_name] = [normalize(item) for item in all_test_pred[file_name]]\n",
    "    \n",
    "    for i, sentence in enumerate(eval_data[file_name]):\n",
    "        evaluator.add_example(sentence, sentence)\n",
    "        \n",
    "report, successes, matches, failure_files = evaluator.evaluateModel(eval_data, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Corpus Matches : 84.78%\n",
      "test Corpus Success : 70.87%\n",
      "Total number of dialogues: 999 \n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = [item[0] for item in failure_files if item[1][1] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMUL2166.json\n",
      "PMUL2436.json\n",
      "MUL1588.json\n",
      "PMUL1323.json\n",
      "SNG0580.json\n",
      "PMUL4440.json\n",
      "PMUL4840.json\n",
      "MUL2525.json\n",
      "SNG1004.json\n",
      "PMUL4660.json\n",
      "PMUL4716.json\n",
      "MUL0071.json\n",
      "MUL2499.json\n",
      "PMUL1173.json\n",
      "PMUL2477.json\n",
      "MUL0671.json\n",
      "MUL2410.json\n",
      "PMUL3044.json\n",
      "MUL1050.json\n",
      "PMUL2634.json\n",
      "PMUL0129.json\n",
      "SNG0571.json\n",
      "PMUL2917.json\n",
      "PMUL3424.json\n",
      "SNG0733.json\n",
      "MUL2365.json\n",
      "PMUL1593.json\n",
      "PMUL0006.json\n",
      "PMUL2882.json\n",
      "PMUL4616.json\n",
      "PMUL0090.json\n",
      "PMUL2563.json\n",
      "PMUL3647.json\n",
      "PMUL1772.json\n",
      "MUL0937.json\n",
      "MUL0148.json\n",
      "MUL2664.json\n",
      "PMUL0204.json\n",
      "MUL1883.json\n",
      "SNG0792.json\n",
      "PMUL2491.json\n",
      "PMUL4155.json\n",
      "MUL0089.json\n",
      "PMUL3992.json\n",
      "MUL2195.json\n",
      "PMUL2119.json\n",
      "PMUL2627.json\n",
      "MUL1675.json\n",
      "MUL0233.json\n",
      "SNG0274.json\n",
      "PMUL2670.json\n",
      "PMUL4255.json\n",
      "MUL2281.json\n",
      "PMUL3336.json\n",
      "MUL1351.json\n",
      "PMUL4059.json\n",
      "PMUL2195.json\n",
      "PMUL4325.json\n",
      "PMUL3940.json\n",
      "PMUL0012.json\n",
      "PMUL2146.json\n",
      "PMUL2945.json\n",
      "PMUL1046.json\n",
      "PMUL1087.json\n",
      "SNG0933.json\n",
      "PMUL4648.json\n",
      "PMUL2437.json\n",
      "MUL2491.json\n",
      "MUL0738.json\n",
      "SNG0964.json\n",
      "PMUL1105.json\n",
      "MUL2151.json\n",
      "PMUL4362.json\n",
      "PMUL1486.json\n",
      "PMUL3523.json\n",
      "MUL2358.json\n",
      "MUL0831.json\n",
      "MUL0890.json\n",
      "SNG01165.json\n",
      "MUL0838.json\n",
      "PMUL3957.json\n",
      "PMUL2983.json\n",
      "SNG0954.json\n",
      "MUL2051.json\n",
      "PMUL0615.json\n",
      "MUL0011.json\n",
      "MUL2665.json\n",
      "PMUL3328.json\n",
      "MUL0570.json\n",
      "PMUL1332.json\n",
      "MUL1766.json\n",
      "PMUL2980.json\n",
      "MUL0761.json\n",
      "PMUL4515.json\n",
      "PMUL2704.json\n",
      "SNG0991.json\n",
      "PMUL4011.json\n",
      "PMUL1600.json\n",
      "MUL2630.json\n",
      "MUL1712.json\n",
      "MUL0014.json\n",
      "SNG0855.json\n",
      "MUL0099.json\n",
      "MUL0941.json\n",
      "SNG01983.json\n",
      "PMUL4078.json\n",
      "MUL1071.json\n",
      "MUL1417.json\n",
      "PMUL1998.json\n",
      "PMUL3275.json\n",
      "PMUL0522.json\n",
      "PMUL1980.json\n",
      "MUL0654.json\n",
      "SNG0649.json\n",
      "PMUL4800.json\n",
      "SNG0611.json\n",
      "MUL2204.json\n",
      "PMUL0205.json\n",
      "MUL1268.json\n",
      "MUL2457.json\n",
      "MUL1055.json\n",
      "MUL2177.json\n",
      "MUL0389.json\n",
      "PMUL0506.json\n",
      "PMUL3364.json\n",
      "PMUL2729.json\n",
      "PMUL4050.json\n",
      "SNG01551.json\n",
      "PMUL3521.json\n",
      "MUL2482.json\n",
      "SNG0779.json\n",
      "MUL2675.json\n",
      "MUL1059.json\n",
      "MUL2637.json\n",
      "PMUL4234.json\n",
      "MUL2316.json\n",
      "PMUL0286.json\n",
      "SNG0822.json\n",
      "SNG0289.json\n",
      "PMUL3935.json\n",
      "SNG0459.json\n",
      "MUL2301.json\n",
      "PMUL4176.json\n",
      "SNG0866.json\n",
      "PMUL2433.json\n",
      "PMUL0768.json\n",
      "PMUL4224.json\n",
      "MUL1756.json\n",
      "MUL1546.json\n",
      "MUL0198.json\n",
      "PMUL4930.json\n",
      "PMUL1008.json\n"
     ]
    }
   ],
   "source": [
    "for item in failures:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usr:hi , can you give me some information on a place to stay when i arrive in cambridge next week ?\n",
      "Sys:there are [value_choice] [value_type] and [value_choice] [value_type] [value_area] . what type of place and price range are you looking for ?\n",
      "Usr:not worried about the price but i would like a guest house in the east with free wifi .\n",
      "Sys:the [value_name] is in the [value_area] with free wifi . would you like their address ?\n",
      "Usr:yes , and may you please book a room in the hotel for 4 nights ? it is for 1 person .\n",
      "Sys:i can help you with that . what day will you be arriving at the [value_type] ?\n",
      "Usr:i will be arriving on monday , i will need the reference number please .\n",
      "Sys:the address for the [value_name] is [value_address] . your reservation has been set for check - in on [value_day] and your reference number is [value_reference] .\n",
      "Usr:i am also looking for some place -s to go in the same area as the hotel . can you make some suggestions ? i need the address , postcode , and entrance fee if any .\n",
      "Sys:what type of attraction are you looking for ?\n",
      "Usr:how about some restaurant -s , and i want to book a taxi so that i can travel from place to place .\n",
      "Sys:i apologize . i do n't want to misunderstand your request . so , you are wanting to look for an attraction or a restaurant ?\n",
      "Usr:sorry , i am looking for an attraction .\n",
      "Sys:the [value_name] is located at [value_address] [value_postcode] . it has free admission .\n",
      "Usr:do they have ample parking ?\n",
      "Sys:i am sorry i do n't have that information .\n",
      "Usr:that is fine , uh ... i want to book a taxi so that i can travel from the guest house to the museum .\n",
      "Sys:what day and time would you like to book a taxi for ?\n",
      "Usr:i would like to leave the museum by 23:15 to head back to the guest house .\n",
      "Sys:okay , your driver will be in a [value_car] and the contact number is [value_phone] . can i just confirm , you are traveling back to the [value_destination] from the [value_departure] ?\n",
      "Usr:great thank you . yes that is correct .\n",
      "Sys:perfect . can i assist you with anything else today ?\n",
      "Usr:no that will be all for now . thank you .\n",
      "Sys:you are welcome , let me know if i can assist with anything else .\n"
     ]
    }
   ],
   "source": [
    "filename = 'PMUL2882.json'\n",
    "\n",
    "for i in range(len(test_data[filename]['log'])):\n",
    "    print(\"Usr:\" + test_data[filename]['log'][i]['user'])\n",
    "    print(\"Sys:\" + test_data[filename]['log'][i]['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usr: hi , can you give me some information on a place to stay when i arrive in cambridge next week ?\n",
      "Sys: sure , do you have a price range in mind ?\n",
      "Usr: not worried about the price but i would like a guest house in the east with free wifi .\n",
      "Sys: i have [value_count] [value_type] in the [value_area] with free wifi . do you have a preference for price range ?\n",
      "Usr: yes , and may you please book a room in the hotel for 4 nights ? it is for 1 person .\n",
      "Sys: what day will you be checking in ?\n",
      "Usr: i will be arriving on monday , i will need the reference number please .\n",
      "Sys: i have booked you a room at [hotel_name] . your reference number is [hotel_reference] .\n",
      "Usr: i am also looking for some place -s to go in the same area as the hotel . can you make some suggestions ? i need the address , postcode , and entrance fee if any .\n",
      "Sys: i am sorry , i am not able to find any attractions in the [value_area] .\n",
      "Usr: how about some restaurant -s , and i want to book a taxi so that i can travel from place to place .\n",
      "Sys: i am sorry , i do n't understand . can you please restate your requirement -s ?\n",
      "Usr: sorry , i am looking for an attraction .\n",
      "Sys: there are [value_count] attractions in the [value_area] . do you have a preference for type ?\n",
      "Usr: do they have ample parking ?\n",
      "Sys: yes , it does have free parking .\n",
      "Usr: that is fine , uh ... i want to book a taxi so that i can travel from the guest house to the museum .\n",
      "Sys: i have booked a [taxi_type] for you . the contact number is [taxi_phone] .\n",
      "Usr: i would like to leave the museum by 23:15 to head back to the guest house .\n",
      "Sys: i have booked you a [taxi_type] and the contact number is [taxi_phone] .\n",
      "Usr: great thank you . yes that is correct .\n",
      "Sys: is there anything else i can help you with ?\n",
      "Usr: no that will be all for now . thank you .\n",
      "Sys: thank you for using our service . have a great day .\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_data[filename]['log'])):\n",
    "    print(\"Usr: \" + test_data[filename]['log'][i]['user'])\n",
    "    print(\"Sys: \" + eval_data[filename][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'taxi': {'info': {'leaveat': '23:15'},\n",
       "  'reqt': ['car', 'phone'],\n",
       "  'fail_info': {}},\n",
       " 'hotel': {'info': {'area': 'east', 'type': 'guesthouse', 'internet': 'yes'},\n",
       "  'fail_info': {},\n",
       "  'book': {'people': '1', 'day': 'monday', 'invalid': False, 'stay': '4'},\n",
       "  'fail_book': {}},\n",
       " 'attraction': {'info': {'area': 'east'},\n",
       "  'reqt': ['price', 'address', 'postcode'],\n",
       "  'fail_info': {}}}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[filename]['goal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dialog = test_data[file_name]['log']\n",
    "\n",
    "full_dialog_tokens = []\n",
    "cur_pos = 0\n",
    "\n",
    "for turn_dialog in full_dialog:\n",
    "    # cur_pos = 0\n",
    "\n",
    "    # user\n",
    "    user = recoverText(turn_dialog['user_delex'])\n",
    "    # user = recoverText(turn_dialog['user_delex'])\n",
    "    user_tokens = self.user_bos + tokenizer.encode(user) + self.eos\n",
    "\n",
    "    user_pos = torch.arange(cur_pos, cur_pos + len(user_tokens))\n",
    "    cur_pos = user_pos[-1] + 1\n",
    "\n",
    "    # belief span\n",
    "#             belief_tokens = self.bos + \\\n",
    "#                             tokenizer.encode(\";\".join(turn_dialog['bspan_inform'][1:])) + \\\n",
    "#                             self.eos\n",
    "#             belief_pos = torch.arange(cur_pos, cur_pos + len(belief_tokens))\n",
    "#             cur_pos = belief_pos[-1]\n",
    "\n",
    "\n",
    "    # Database\n",
    "    if eval(turn_dialog['pointer'])[-2:] == (1, 0):\n",
    "        booked = \"book\"\n",
    "    elif eval(turn_dialog['pointer'])[-2:] == (0, 1):\n",
    "        booked = \"fail\"\n",
    "    else:\n",
    "        booked = \"none\"\n",
    "\n",
    "    if len(turn_dialog['match']) > 0:\n",
    "        num_match = int(turn_dialog['match']) if int(turn_dialog['match']) < 4 else 4\n",
    "    else:\n",
    "        num_match = 0\n",
    "\n",
    "    database = str(num_match) + \";\" + booked + \";\" + turn_dialog['turn_domain'].strip(\"[]\") + \";\"\n",
    "    database_tokens = tokenizer.encode(database)\n",
    "    database_pos = torch.arange(cur_pos, cur_pos + len(database_tokens))\n",
    "    cur_pos = database_pos[-1] + 1\n",
    "\n",
    "    # System\n",
    "    system = recoverText(process_text(turn_dialog['resp'], turn_dialog['turn_domain'].strip(\"[]\")))\n",
    "    system_tokens = self.system_bos + tokenizer.encode(system) + self.eos\n",
    "    system_pos = torch.arange(cur_pos, cur_pos + len(system_tokens))\n",
    "    cur_pos = system_pos[-1] + 1\n",
    "\n",
    "    user_tokens = torch.LongTensor(user_tokens)\n",
    "    system_tokens = torch.LongTensor(system_tokens)\n",
    "    database_tokens = torch.LongTensor(database_tokens)\n",
    "\n",
    "    full_dialog_tokens.append((user_tokens, \n",
    "                               user_pos,\n",
    "                               system_tokens, \n",
    "                               system_pos,\n",
    "                               database_tokens,\n",
    "                               database_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data[filename]['log']))\n",
    "len(eval_data[file_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames, _ = zip(*failure_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data['PMUL2859.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/norm-multi-woz/test_dials.json\") as f:\n",
    "    gt_test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item['resp'] for item in test_data['PMUL2859.json']['log']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item['user'] for item in test_data['PMUL2859.json']['log']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item['turn_domain'] for item in test_data['PMUL2859.json']['log']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['PMUL2859.json']['sys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluateModel(eval_data, mode='rollout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_test_pred = {}\n",
    "\n",
    "for file_name in test_data:\n",
    "    rollout_test_pred[file_name] = delex_data[file_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_test_pred = {}\n",
    "\n",
    "for file_name in test_data:\n",
    "    generated_test_pred[file_name] = [item['text'].strip() \n",
    "                                      for i, item in enumerate(delex_data[file_name]['log']) \n",
    "                                      if i % 2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"rollout\"\n",
    "evaluator = MultiWozEvaluator(data_name)\n",
    "\n",
    "evaluator.initialize()\n",
    "eval_data = {}\n",
    "\n",
    "evaluator.evaluateModel(rollout_test_pred, mode='rollout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"test\"\n",
    "evaluator = MultiWozEvaluator(data_name)\n",
    "\n",
    "evaluator.initialize()\n",
    "eval_data = {}\n",
    "\n",
    "evaluator.evaluateModel(generated_test_pred, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
