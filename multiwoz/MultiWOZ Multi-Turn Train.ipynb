{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchfly\n",
    "torchfly.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from apex import amp\n",
    "from allennlp.training.checkpointer import Checkpointer\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule, GPT2Tokenizer\n",
    "\n",
    "from torchfly.criterions import SequenceFocalLoss, SequenceCrossEntropyLoss\n",
    "from gpt_model import GPT2SimpleLM\n",
    "from text_utils import recoverText, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.sep_token = \"None\"\n",
    "# add speicial tokens in the same order as Roberta\n",
    "tokenizer.add_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = GPT2SimpleLM(GPT2SmallConfig)\n",
    "model_B = GPT2SimpleLM(GPT2SmallConfig)\n",
    "model_A.load_state_dict(torch.load(\"gpt2_small.pth\"))\n",
    "model_B.load_state_dict(torch.load(\"gpt2_small.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_keep_indices(batch_keep_indices):\n",
    "    prev = batch_keep_indices[1]\n",
    "    new_batch_keep_indices = [prev]\n",
    "\n",
    "    for i in range(1, len(batch_keep_indices)):\n",
    "        curr = batch_keep_indices[i]\n",
    "        new = []\n",
    "\n",
    "        for idx in curr:\n",
    "            new.append(prev.index(idx))\n",
    "\n",
    "        new_batch_keep_indices.append(new)\n",
    "        prev = curr\n",
    "        \n",
    "    return new_batch_keep_indices\n",
    "\n",
    "\n",
    "class MultiWOZDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bos = tokenizer.encode(\"<s>\")\n",
    "        self.user_bos = tokenizer.encode(\"A:\")\n",
    "        self.system_bos = tokenizer.encode(\"B:\")\n",
    "        \n",
    "        self.eos = [628, 198]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_dialog = self.data[index]['log']\n",
    "        \n",
    "        full_dialog_tokens = []\n",
    "        cur_pos = 0\n",
    "        \n",
    "        for turn_dialog in full_dialog:\n",
    "            # cur_pos = 0\n",
    "            \n",
    "            # user\n",
    "            user = recoverText(turn_dialog['user_delex'])\n",
    "            # user = recoverText(turn_dialog['user_delex'])\n",
    "            user_tokens = self.user_bos + tokenizer.encode(user) + self.eos\n",
    "\n",
    "            user_pos = torch.arange(cur_pos, cur_pos + len(user_tokens))\n",
    "            cur_pos = user_pos[-1] + 1\n",
    "            \n",
    "            # belief span\n",
    "#             belief_tokens = self.bos + \\\n",
    "#                             tokenizer.encode(\";\".join(turn_dialog['bspan_inform'][1:])) + \\\n",
    "#                             self.eos\n",
    "#             belief_pos = torch.arange(cur_pos, cur_pos + len(belief_tokens))\n",
    "#             cur_pos = belief_pos[-1]\n",
    "\n",
    "\n",
    "            # Database\n",
    "            if eval(turn_dialog['pointer'])[-2:] == (1, 0):\n",
    "                booked = \"book\"\n",
    "            elif eval(turn_dialog['pointer'])[-2:] == (0, 1):\n",
    "                booked = \"fail\"\n",
    "            else:\n",
    "                booked = \"none\"\n",
    "            \n",
    "            if len(turn_dialog['match']) > 0:\n",
    "                num_match = int(turn_dialog['match']) if int(turn_dialog['match']) < 4 else 4\n",
    "            else:\n",
    "                num_match = 0\n",
    "                \n",
    "            database = str(num_match) + \";\" + booked + \";\" + turn_dialog['turn_domain'].strip(\"[]\") + \";\"\n",
    "            database_tokens = tokenizer.encode(database)\n",
    "            database_pos = torch.arange(cur_pos, cur_pos + len(database_tokens))\n",
    "            cur_pos = database_pos[-1] + 1\n",
    "            \n",
    "            # System\n",
    "            system = recoverText(process_text(turn_dialog['resp'], turn_dialog['turn_domain'].strip(\"[]\")))\n",
    "            system_tokens = self.system_bos + tokenizer.encode(system) + self.eos\n",
    "            system_pos = torch.arange(cur_pos, cur_pos + len(system_tokens))\n",
    "            cur_pos = system_pos[-1] + 1\n",
    "            \n",
    "            user_tokens = torch.LongTensor(user_tokens)\n",
    "            system_tokens = torch.LongTensor(system_tokens)\n",
    "            database_tokens = torch.LongTensor(database_tokens)\n",
    "            \n",
    "            full_dialog_tokens.append((user_tokens, \n",
    "                                       user_pos,\n",
    "                                       system_tokens, \n",
    "                                       system_pos,\n",
    "                                       database_tokens,\n",
    "                                       database_pos))\n",
    "#             if system_pos[-1] > 1:\n",
    "#                 break\n",
    "\n",
    "        return full_dialog_tokens\n",
    "        \n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = self.tokenizer.encode(\"<pad>\")[0]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "\n",
    "        max_turn_len = max([len(item) for item in unpacked_data])\n",
    "        \n",
    "        batch_dialogs = []\n",
    "        batch_keep_indices = []\n",
    "\n",
    "        for turn_num in range(max_turn_len):\n",
    "\n",
    "            keep_indices = []\n",
    "\n",
    "            for batch_idx in range(len(unpacked_data)):\n",
    "                if turn_num < len(unpacked_data[batch_idx]):\n",
    "                    keep_indices.append(batch_idx)\n",
    "\n",
    "            user_tokens = pad_sequence([unpacked_data[idx][turn_num][0] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            user_pos = pad_sequence([unpacked_data[idx][turn_num][1] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            system_tokens = pad_sequence([unpacked_data[idx][turn_num][2] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            system_pos = pad_sequence([unpacked_data[idx][turn_num][3] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            database_tokens = pad_sequence([unpacked_data[idx][turn_num][4] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            database_pos = pad_sequence([unpacked_data[idx][turn_num][5] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)  \n",
    "\n",
    "            user_mask = (user_tokens != self.pad).byte()\n",
    "            system_mask = (system_tokens != self.pad).byte()\n",
    "            database_mask = (database_tokens != self.pad).byte()\n",
    "\n",
    "\n",
    "            batch_dialogs.append((user_tokens, user_pos, user_mask, \n",
    "                                  system_tokens, system_pos, system_mask, \n",
    "                                  database_tokens, database_pos, database_mask))\n",
    "            batch_keep_indices.append(keep_indices)\n",
    "            \n",
    "        # align keep indices\n",
    "        # batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "        return batch_dialogs, batch_keep_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, target, mask):\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    target = target[:, 1:].contiguous()\n",
    "    mask = mask[:, 1:].contiguous().float()\n",
    "    loss = criterion(logits, target, mask, label_smoothing=0.01, reduce=True)\n",
    "    return loss\n",
    "\n",
    "def filter_past(past, keep_indices):\n",
    "    past = [item[:, keep_indices] for item in past]\n",
    "    return past\n",
    "\n",
    "def replace_punc(x):\n",
    "    x = x.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    return x.replace(\".\", \" .\").replace(\",\", \" .\").replace(\"?\", \" ?\").replace(\"?\", \" ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, domain):\n",
    "    text = text.replace(\"[value_choice]\", \"[value_count]\")\n",
    "    text = text.replace(\"[value_people]\", \"[value_count]\")\n",
    "    text = text.replace(\"[value_starts]\", \"[value_count]\")\n",
    "    \n",
    "    text = text.replace(\"[value_car]\", '[taxi_type]')\n",
    "    text = text.replace(\"[value_leave]\", \"[value_time]\")\n",
    "    text = text.replace(\"[value_arrive]\", \"[value_time]\")\n",
    "    text = text.replace(\"[value_price]\", \"[value_pricerange]\")\n",
    "\n",
    "    text = text.replace('[value_postcode]', f'[{domain}_postcode]')\n",
    "    text = text.replace('[value_reference]', f'[{domain}_reference]')\n",
    "    text = text.replace('[value_address]', f'[{domain}_address]')\n",
    "    text = text.replace('[value_phone]', f'[{domain}_phone]')\n",
    "    text = text.replace('[value_name]', f'[{domain}_name]')\n",
    "    text = text.replace('[value_id]', f'[{domain}_id]')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../yichi_data/clean_train_data.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(\"../yichi_data/val_data.json\") as f:\n",
    "    val_data = json.load(f)\n",
    "    \n",
    "with open(\"../yichi_data/test_data.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "indices = np.arange(len(train_data))\n",
    "np.random.shuffle(indices)\n",
    "# use all data\n",
    "indices = indices\n",
    "train_data = [train_data[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiWOZDataset(train_data, tokenizer)\n",
    "val_dataset = MultiWOZDataset(val_data, tokenizer)\n",
    "test_dataset = MultiWOZDataset(test_data, tokenizer)\n",
    "\n",
    "train_batch_size = 1\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True,\n",
    "                              batch_size=train_batch_size, \n",
    "                              collate_fn=collate_func)\n",
    "\n",
    "eval_batch_size = 4\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SequenceFocalLoss(gamma=0.0, beta=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_B.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = Checkpointer(serialization_dir=\"Checkpoint\", \n",
    "                            keep_serialized_model_every_num_seconds=3600*2, \n",
    "                            num_serialized_models_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "num_epochs = 20\n",
    "num_gradients_accumulation = 4\n",
    "num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // train_batch_size // num_gradients_accumulation\n",
    "\n",
    "param_optimizer = list(model_A.named_parameters()) + list(model_B.named_parameters())\n",
    "no_decay = ['ln', 'bias', 'LayerNorm']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=1e-4,\n",
    "                  correct_bias=False)\n",
    "\n",
    "scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                 warmup_steps=1000,\n",
    "                                 t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_weight = 1.0\n",
    "\n",
    "def train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False):\n",
    "\n",
    "    aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "   \n",
    "    mask = torch.ByteTensor([]).to(device)\n",
    "    prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "    \n",
    "    past = None\n",
    "    all_logits = []\n",
    "    target = []\n",
    "    total_loss = 0 \n",
    "    \n",
    "    for turn_num in range(len(batch_keep_indices)):\n",
    "\n",
    "        # data send to gpu\n",
    "        dialogs = batch_dialogs[turn_num]\n",
    "        dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "        user_tokens, user_pos, user_mask, \\\n",
    "            system_tokens, system_pos, system_mask, \\\n",
    "            database_tokens, database_pos, database_mask = dialogs\n",
    "\n",
    "        # filtering algorithm\n",
    "        keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "        if len(keep_indices) != prev_batch_size:\n",
    "            past = filter_past(past, keep_indices)\n",
    "            mask = mask[keep_indices, :]\n",
    "\n",
    "        # User Utterance\n",
    "        mask = torch.cat([mask, user_mask], dim=-1)\n",
    "        logits, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "        A_loss = calculate_loss(logits, user_tokens, user_mask)\n",
    "\n",
    "        # Database Tokens\n",
    "        mask = torch.cat([mask, database_mask], dim=-1)\n",
    "        logits, past = model_B(database_tokens, position_ids=database_pos, mask=mask, past=past)\n",
    "        database_loss = calculate_loss(logits, database_tokens, database_mask)        \n",
    "        \n",
    "        # System Response\n",
    "        mask = torch.cat([mask, system_mask], dim=-1)\n",
    "        logits, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "        B_loss = calculate_loss(logits, system_tokens, system_mask)\n",
    "\n",
    "        # tail\n",
    "        total_loss = total_loss + user_weight * A_loss + B_loss + database_loss\n",
    "        prev_batch_size = user_tokens.shape[0]\n",
    "\n",
    "#     breakpoint\n",
    "#     all_logits = torch.cat(all_logits, dim=1)\n",
    "#     all_logits = all_logits[:, :-1].contiguous()\n",
    "\n",
    "#     target = torch.cat(target, dim=1)\n",
    "#     target = target[:, 1:].contiguous()\n",
    "    \n",
    "#     target_mask = torch.ones_like(target).float()\n",
    "    \n",
    "#     total_loss = criterion(all_logits, target, target_mask, label_smoothing=0.02, reduce=True)\n",
    "\n",
    "    # gradient accumulation\n",
    "    total_loss /= len(batch_keep_indices)\n",
    "    total_loss /= num_gradients_accumulation \n",
    "    \n",
    "    if fp16:\n",
    "        with amp.scale_loss(total_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    else:\n",
    "        total_loss.backward()\n",
    "        \n",
    "    record_loss = total_loss.item() * num_gradients_accumulation\n",
    "    perplexity = np.exp(record_loss)\n",
    "    \n",
    "    return record_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_length(batch_dialogs):\n",
    "    total_sum = 0\n",
    "    for turn_num in range(len(batch_keep_indices)):\n",
    "        total_sum += batch_dialogs[turn_num][2].sum(-1) + \\\n",
    "                        batch_dialogs[turn_num][5].sum(-1) + \\\n",
    "                        batch_dialogs[turn_num][8].sum(-1)\n",
    "    return total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64c41b1901c4e57bc964bc73584ec6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41b08cb84854fc8aa34b93dd3389688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fe489ae830441591ed7d4f3da61047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d594ea28404444da5cd794c2e7aa9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f65f320f3174a3895dd9dd696759a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79c754c91b642de9ee873bde54105d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c43ec4975b4872822fce8665199c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04359182b5b64fe5a3d7b329be4f9f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf804a0a8da54baea86df46220e4592b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d87e6822a2b49079a8e6ce1ba29c6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eded74559074f54ba22b741700745fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a89fb8eb194cb88934fbd12f35813c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e40d8a1ad7c4f4cad6a88a848bc862c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1cc5e81d3b40129d7be248e7c740ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db0347c36434065811ce6c68d06c05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a67936048d940e8b0423e47fb9bfbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6cd7e1c78d4ebaa3be07422eed982a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19290c6f9fe4148af6f95837ab81bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f848e06db774b3eb28c6b974d8c4297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5c057ffee2430ea18417df2293e6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "exceed limit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "progress_bar = tqdm.tqdm_notebook\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    \"Training\"\n",
    "    pbar = progress_bar(train_dataloader)\n",
    "    model_A.train()\n",
    "    model_B.train()\n",
    "    \n",
    "    for batch_dialogs, batch_keep_indices in pbar:\n",
    "        \n",
    "        if calculate_length(batch_dialogs).item() > 900:\n",
    "            print(\"exceed limit\")\n",
    "            continue\n",
    "            \n",
    "        if len(batch_keep_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        record_loss, perplexity = train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False)\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:\n",
    "            # update for gradient accumulation\n",
    "            scheduler.step()\n",
    "            torch.nn.utils.clip_grad_norm_(model_A.parameters(), 5.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model_B.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # speed measure\n",
    "            end = time.time()\n",
    "            speed = train_batch_size * num_gradients_accumulation / (end - start)\n",
    "            start = end\n",
    "            \n",
    "            # show progress\n",
    "            pbar.set_postfix(loss=record_loss, perplexity=perplexity, speed=speed)\n",
    "    \n",
    "#     \"Evaluation\"\n",
    "#     print(f\"Epoch {ep} Validation\")\n",
    "#     eval_res = validate(val_dataloader, val_data)\n",
    "#     print(eval_res)\n",
    "    \n",
    "#     print(f\"Epoch {ep} Test\")\n",
    "#     eval_res = validate(test_dataloader, test_data)\n",
    "#     print(eval_res)\n",
    "    \n",
    "    checkpointer.save_checkpoint(ep, \n",
    "                                 [model_A.state_dict(), model_B.state_dict()],\n",
    "                                 {\"None\": None},\n",
    "                                 True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for batch_dialogs, batch_keep_indices in pbar:\n",
    "    length = calculate_length(batch_dialogs).item()\n",
    "    res.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, data):\n",
    "\n",
    "    model_A.eval()\n",
    "    model_B.eval()\n",
    "\n",
    "    temperature = 0.5\n",
    "\n",
    "    all_response = []\n",
    "\n",
    "    for batch_dialogs, batch_keep_indices in tqdm.tqdm_notebook(dataloader):\n",
    "\n",
    "        aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "        past = None\n",
    "        generated_responses = [[] for i in range(batch_dialogs[0][0].shape[0])]\n",
    "\n",
    "        mask = torch.ByteTensor([]).to(device)\n",
    "        prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for turn_num in range(len(batch_keep_indices)):\n",
    "                # data send to gpu\n",
    "                dialogs = batch_dialogs[turn_num]\n",
    "                dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "                user_tokens, user_pos, user_mask, \\\n",
    "                    system_tokens, system_pos, system_mask, \\\n",
    "                    belief_tokens, belief_pos, belief_mask = dialogs\n",
    "\n",
    "                # batch filtering algorithm\n",
    "                keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "                if len(keep_indices) != prev_batch_size:\n",
    "                    past = filter_past(past, keep_indices)\n",
    "                    mask = mask[keep_indices, :]\n",
    "\n",
    "                # define some initials\n",
    "                cur_batch_size = user_tokens.shape[0]\n",
    "                flags = np.ones(cur_batch_size)\n",
    "                generated_tokens = [[] for i in range(cur_batch_size)]\n",
    "\n",
    "                # feed in user\n",
    "                mask = torch.cat([mask, user_mask], dim=-1)\n",
    "                _, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "\n",
    "                # response generation\n",
    "                response = []\n",
    "\n",
    "\n",
    "                # first three tokens\n",
    "                prev_input = system_tokens[:, :3]\n",
    "                cur_pos = system_pos[:, :3]\n",
    "                temp_past = past\n",
    "                temp_mask = F.pad(mask, pad=(0,3), value=1)\n",
    "\n",
    "                # feed into B\n",
    "                logits, temp_past = model_B(prev_input, position_ids=cur_pos, mask=temp_mask, past=temp_past)\n",
    "                # set current position\n",
    "                cur_pos = cur_pos[:, -1].unsqueeze(1) + 1\n",
    "\n",
    "                for i in range(50):\n",
    "                    logits = logits[:, -1, :] / temperature\n",
    "                    prev_tokens = torch.argmax(logits, dim=-1)\n",
    "                    np_prev_tokens = prev_tokens.cpu().numpy()\n",
    "                    # nucleus sampling\n",
    "                    # logits = top_filtering(logits, top_k=100, top_p=0.7)\n",
    "                    # probs = F.softmax(logits, -1)\n",
    "                    # prev_input = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "                    # add to generated tokens list\n",
    "                    count = 0\n",
    "                    for idx, value in enumerate(flags):\n",
    "                        if value != 0:\n",
    "                            generated_tokens[idx].append(np_prev_tokens[count])\n",
    "                            count += 1\n",
    "\n",
    "                    # filtering algorithm\n",
    "                    if np.any(np_prev_tokens == 628):\n",
    "                        # set flags 0\n",
    "                        count = 0\n",
    "                        for idx, value in enumerate(flags):\n",
    "                            if value == 1:\n",
    "                                if np_prev_tokens[count] == 628:\n",
    "                                    flags[idx] = 0\n",
    "                                count += 1\n",
    "                        # compute which one to keep\n",
    "                        keep_indices = np.argwhere(np_prev_tokens != 628).squeeze(1)\n",
    "                        # filter\n",
    "                        prev_tokens = prev_tokens[keep_indices.tolist()]\n",
    "                        cur_pos = cur_pos[keep_indices.tolist(), :]\n",
    "                        temp_mask = temp_mask[keep_indices.tolist(), :]\n",
    "                        temp_past = [item[:, keep_indices.tolist()] for item in temp_past]\n",
    "                        np_prev_tokens = np_prev_tokens[keep_indices.tolist()]\n",
    "\n",
    "                    if np.all(flags == 0):\n",
    "                        break\n",
    "\n",
    "                    # prepare for the next token        \n",
    "                    temp_mask = F.pad(temp_mask, pad=(0, 1), value=1)\n",
    "                    logits, temp_past = model_B(prev_tokens.view(-1, 1), \n",
    "                                           position_ids=cur_pos, \n",
    "                                           mask=temp_mask, \n",
    "                                           past=temp_past)\n",
    "                    cur_pos = cur_pos + 1\n",
    "\n",
    "                # real system_tokens feed in\n",
    "                mask = torch.cat([mask, system_mask], dim=-1)\n",
    "                _, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "\n",
    "                # inject into generated_responses_list\n",
    "                decoded_responses = [tokenizer.decode(item).replace(\"\\n\", \"\") for item in generated_tokens]\n",
    "                count = 0\n",
    "                for idx in batch_keep_indices[turn_num]:\n",
    "                    generated_responses[idx].append(decoded_responses[count])\n",
    "                    count += 1\n",
    "\n",
    "            # add to the final responses        \n",
    "            for item in generated_responses:\n",
    "                all_response.extend(item)\n",
    "                \n",
    "    # Stage 2\n",
    "    #   prepare for metric eval\n",
    "    dialog_data = []\n",
    "    count = 0\n",
    "    all_results = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        raw_dialog = data[i]\n",
    "\n",
    "        for turn_num in range(len(raw_dialog)):\n",
    "\n",
    "            replaced_response = clean_sentence(\n",
    "                replace_punc(raw_dialog[turn_num][\"replaced_response\"].lower().replace(\"slot\", \"SLOT\")), entity_dict)\n",
    "\n",
    "            generated_response = clean_sentence(replace_punc(all_response[count].lower().replace(\"slot\", \"SLOT\")), entity_dict)\n",
    "\n",
    "            dialog_data.append({\"dial_id\": raw_dialog[turn_num][\"dial_id\"],\n",
    "                                \"turn_num\": raw_dialog[turn_num][\"turn_num\"],\n",
    "                                \"response\": replaced_response,\n",
    "                                \"generated_response\":generated_response \n",
    "                              })\n",
    "            count += 1\n",
    "            \n",
    "    sccuess_f1 = success_f1_metric(dialog_data)\n",
    "    bleu = bleu_metric(dialog_data)\n",
    "\n",
    "    return {\"bleu\": bleu,\n",
    "            \"sccuess_f1\": sccuess_f1\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
