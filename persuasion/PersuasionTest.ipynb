{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from apex import amp\n",
    "from gpt_model import GPT2SimpleLM\n",
    "from pytorch_pretrained_bert import GPT2Tokenizer, OpenAIAdam\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "\n",
    "from torchfly.criterions import SequenceFocalLoss, SequenceCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersuadeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.max_len = 1500\n",
    "        self.turn_ending = tokenizer.encode(\"\\n\\n\\n\")\n",
    "        self.dialog_ending = [tokenizer.encoder[\"[EOS]\"]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]\n",
    "        role_ids = [0 if item[0] == 32 else 1 for item in dial_tokens]\n",
    "        dial_tokens[-1] = dial_tokens[-1][:-2] + self.dialog_ending\n",
    "        return role_ids, dial_tokens\n",
    "        \n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.EOS = self.tokenizer.encoder[\"[EOS]\"]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "        return unpacked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "    \"\"\"\n",
    "    # batch support!\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_values = values[:, -1].unsqueeze(1).repeat(1, logits.shape[-1])\n",
    "        logits = torch.where(logits < min_values, \n",
    "                             torch.ones_like(logits, dtype=logits.dtype) * -float('Inf'), \n",
    "                             logits)\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        sorted_logits = sorted_logits.masked_fill_(sorted_indices_to_remove, filter_value)\n",
    "        logits = torch.zeros_like(logits).scatter(1, sorted_indices, sorted_logits)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torch.load(\"special3_gpt2_tokenizer.pkl\")\n",
    "\n",
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.__special_tokens__)\n",
    "    n_special = len(tokenizer.__special_tokens__)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.__special_tokens__)\n",
    "    n_special = len(tokenizer.__special_tokens__)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58, 3672, 62, 43384, 60]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[name_slot]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slot'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([43384])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"../DataProcess/train_dialogs.pkl\")\n",
    "val_data = torch.load(\"../DataProcess/val_dialogs.pkl\")\n",
    "\n",
    "train_dataset = PersuadeDataset(train_data, tokenizer)\n",
    "val_dataset = PersuadeDataset(val_data, tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True, \n",
    "                              batch_size=batch_size, \n",
    "                              collate_fn=collate_func)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                            shuffle=False, \n",
    "                            batch_size=batch_size, \n",
    "                            collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_A = GPT2SimpleLM(GPT2SmallConfig)\n",
    "# model_B = GPT2SimpleLM(GPT2SmallConfig)\n",
    "# model_A_states, model_B_states = torch.load(\"CheckpointMedium/model_state_epoch_3.th\")\n",
    "\n",
    "model_A = GPT2SimpleLM(GPT2MediumConfig)\n",
    "model_B = GPT2SimpleLM(GPT2MediumConfig)\n",
    "model_A_states, model_B_states = torch.load(\"persuasion_medium_3.th\")\n",
    "\n",
    "model_A.load_state_dict(model_A_states)\n",
    "model_B.load_state_dict(model_B_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_B.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:Hello! How are you today?\n",
      "B:Hi\n",
      "A:I'm good. How are you doing today?\n",
      "B:quit\n"
     ]
    }
   ],
   "source": [
    "model_A.eval()\n",
    "model_B.eval()\n",
    "\n",
    "prev_input = tokenizer.encode(\"A:\")\n",
    "prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)\n",
    "# past_position_ids = torch.LongTensor([[0, 1]]).to(device)\n",
    "\n",
    "temperature = 0.8\n",
    "past = None\n",
    "flag = True\n",
    "\n",
    "sep = tokenizer.encode(\"\\n\\n\\n\")\n",
    "\n",
    "while flag:\n",
    "    \"Sampling based method\"\n",
    "    sent = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(200):\n",
    "            logits, past = model_A(prev_input, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_filtering(logits, top_k=200, top_p=0.8)\n",
    "            # prev_input = logits.argmax(-1).unsqueeze(1)\n",
    "            probs = F.softmax(logits, -1)\n",
    "            prev_input = torch.multinomial(probs, num_samples=1)\n",
    "            prev_word = prev_input.item()\n",
    "\n",
    "            if prev_word == 628:\n",
    "                break\n",
    "            elif prev_word == tokenizer.encoder[\"[EOS]\"]:\n",
    "                flag = False\n",
    "                break\n",
    "            else:\n",
    "                sent.append(prev_word)\n",
    "            \n",
    "            # past_position_ids = past_position_ids[:, -1:] + 1\n",
    "\n",
    "    if not flag:\n",
    "        break\n",
    "\n",
    "    print(\"A:\" + tokenizer.decode(sent))\n",
    "    \n",
    "    # finish tail\n",
    "    prev_input = torch.LongTensor(sep).unsqueeze(0).to(device)\n",
    "    _, past = model_A(prev_input, past=past)\n",
    "    \n",
    "    # input and update B's utterance\n",
    "    user = input(\"B:\")\n",
    "    \n",
    "    if user == \"quit\":\n",
    "        break\n",
    "        \n",
    "    user = tokenizer.encode(\"B:\" + user)\n",
    "    prev_input = user + sep\n",
    "    prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)\n",
    "    \n",
    "    _, past = model_B(prev_input, past=past)\n",
    "    \n",
    "    # start A's utterance\n",
    "    suffix = tokenizer.encode(\"A:\")\n",
    "    prev_input = torch.LongTensor(suffix).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
